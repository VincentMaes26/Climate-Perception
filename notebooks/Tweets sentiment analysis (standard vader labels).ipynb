{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:36:59.308224Z",
     "start_time": "2019-10-10T13:36:59.303235Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataframe from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:37:01.146166Z",
     "start_time": "2019-10-10T13:37:01.114251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "#tweets_today = pd.read_csv(r\"..\\datasets\\raw_data\\tweets{}.csv\".format(datetime.date.today()))\n",
    "tweets_today = pd.read_csv(r\"..\\datasets\\raw_data\\tweets2019-10-08.csv\")\n",
    "tweets_today = tweets_today.head(5000)\n",
    "# Gets tweet texts\n",
    "tweets_today_text = tweets_today[\"tweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:01.130167Z",
     "start_time": "2019-10-10T13:52:00.086518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing of tweet texts\n",
    "def format_tweet(tweet):\n",
    "    processed_tweet=\"\"\n",
    "    for word in tweet.split():\n",
    "        # Removing URL from tweet\n",
    "        processed_word = re.sub('([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', word)\n",
    "        # remove all single characters\n",
    "        processed_word = re.sub('\\s+[a-zA-Z]\\s+', ' ', processed_word)\n",
    "        # Remove single characters from the start\n",
    "        processed_word = re.sub('\\^[a-zA-Z]\\s+', ' ', processed_word) \n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_word = re.sub('\\s+', '', processed_word, flags=re.I)\n",
    "        # Removing prefixed 'b'\n",
    "        processed_word = re.sub('^b\\s+', ' ', processed_word)\n",
    "        # Removing &amp\n",
    "        processed_word = re.sub('&amp', '&', processed_word)\n",
    "        # Removing breaks\n",
    "        processed_word = re.sub('<br/>', '', processed_word)\n",
    "        # converts to lower\n",
    "        processed_word = processed_word.lower()\n",
    "        processed_tweet= processed_tweet+\" \"+processed_word\n",
    "    return processed_tweet        \n",
    "    \n",
    "tweets_today_text = [format_tweet(tweet) for tweet in tweets_today_text]\n",
    "tweets_today_text = list(filter(None, tweets_today_text))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling tweets with NLTK vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:09.618119Z",
     "start_time": "2019-10-10T13:52:08.149150Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:09.724173Z",
     "start_time": "2019-10-10T13:52:09.691919Z"
    }
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for tweet in tweets_today_text[5:10]:\n",
    "    print(\"{0}: \\n{1}\\n\\n\".format(tweet, sid.polarity_scores(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:32.749457Z",
     "start_time": "2019-10-10T13:52:31.164659Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    if polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "polarity_tweets = [round(sid.polarity_scores(tweet)[\"compound\"], 2)\n",
    "                   for tweet in tweets_today_text]\n",
    "sentiment_tweets = [get_sentiment(polarity) for polarity in polarity_tweets]\n",
    "zipped_list = list(zip(sentiment_tweets, polarity_tweets,\n",
    "                       tweets_today['tweet'], tweets_today['username'], tweets_today['creation date']))\n",
    "\n",
    "# Store to new dataframe\n",
    "sentiment_df = pd.DataFrame(zipped_list, columns=[\n",
    "    \"Sentiment\", \"Polarity\", \"Tweet\", \"Username\", \"Creation Date\"])\n",
    "sentiment_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:33.051211Z",
     "start_time": "2019-10-10T13:52:32.905008Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_count = round(sentiment_df[\"Polarity\"].value_counts(), 2)\n",
    "\n",
    "sentiments = [get_sentiment(polarity)\n",
    "              for polarity in sentiment_df[\"Polarity\"]]\n",
    "\n",
    "sentiment_count_list = np.array([sentiments.count(\n",
    "    \"positive\"), sentiments.count(\"neutral\"), sentiments.count(\"negative\")])\n",
    "\n",
    "labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "colors = [\"green\", \"yellow\", \"red\"]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sentiment_count_list, labels=labels, shadow=True,\n",
    "        autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "ax1.axis('equal')\n",
    "plt.title(\"Distribution of sentiment values\")\n",
    "plt.show()\n",
    "print(\"total positive: {}\".format(sentiments.count('positive')))\n",
    "print(\"total neutral: {}\".format(sentiments.count('neutral')))\n",
    "print(\"total negative: {}\".format(sentiments.count('negative')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this nltk vader implementation for sentiment analysis, we get entirely diferent results. There are way less neutral tweets and more than half of the tweets now have a negative sentiment assigned to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:34.061744Z",
     "start_time": "2019-10-10T13:52:33.859931Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_df[\"Polarity\"].hist(bins=5)\n",
    "plt.title(\"Distribution of sentiment polarities\")\n",
    "plt.xlabel(\"Sentiment polarities\")\n",
    "plt.ylabel(\"Amount of tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:41.525695Z",
     "start_time": "2019-10-10T13:52:41.433699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:42.718993Z",
     "start_time": "2019-10-10T13:52:42.709019Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features from df\n",
    "features = sentiment_df[\"Tweet\"]\n",
    "\n",
    "# Extract labels from df\n",
    "labels = sentiment_df[\"Sentiment\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    labels, test_size=0.2,\n",
    "                                                    random_state=1)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing pipeline variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:48.084939Z",
     "start_time": "2019-10-10T13:52:48.022099Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:52:49.690081Z",
     "start_time": "2019-10-10T13:52:49.670133Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:59:01.913879Z",
     "start_time": "2019-10-10T13:53:52.634289Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_pipeline = Pipeline([('vect', tfidf), ('clf', MultinomialNB())])\n",
    "\n",
    "param_grid_mnb = {\n",
    "                  'vect__use_idf': [True, False],\n",
    "                  'vect__ngram_range': [(1,2)],\n",
    "                  'vect__stop_words': [stopwords, None],\n",
    "                  'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                  'clf__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "                  'clf__fit_prior': [True, False]\n",
    "                 }\n",
    "grid_mnb = RandomizedSearchCV(mnb_pipeline, param_grid_mnb, n_iter=80, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:59:02.508322Z",
     "start_time": "2019-10-10T13:59:02.500337Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", grid_mnb.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}%\".format(grid_mnb.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T12:51:45.749136Z",
     "start_time": "2019-10-10T12:51:42.201603Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb = grid_mnb.best_estimator_\n",
    "mnb.fit(X_train, y_train)\n",
    "predictions_mnb = mnb.predict(X_test)\n",
    "print(confusion_matrix(y_test, predictions_mnb))\n",
    "print(classification_report(y_test, predictions_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T12:51:45.922684Z",
     "start_time": "2019-10-10T12:51:45.918683Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:26:29.632562Z",
     "start_time": "2019-10-10T12:52:16.718136Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_pipeline = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=1))])\n",
    "\n",
    "param_grid_lr ={\n",
    "                'vect__use_idf': [True, False],\n",
    "                'vect__ngram_range': [(1,1),(1,2)],\n",
    "                'vect__stop_words': [stopwords, None],\n",
    "                'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                'clf__dual': [True, False],\n",
    "                'clf__tol': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2 ],\n",
    "                'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                'clf__fit_intercept': [True, False],\n",
    "                'clf__max_iter' : [100, 110, 120, 130, 140],\n",
    "                'clf__warm_start': [True, False]\n",
    "              }\n",
    "grid_lr = RandomizedSearchCV(lr_pipeline, param_grid_lr, n_iter=500,  verbose=2, cv=5, n_jobs=-1)\n",
    "grid_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:26:33.650826Z",
     "start_time": "2019-10-10T13:26:33.637862Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", grid_lr.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}%\".format(grid_lr.best_score_ *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:26:37.693025Z",
     "start_time": "2019-10-10T13:26:33.804416Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = grid_lr.best_estimator_\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "predictions_lr = lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions_lr))\n",
    "print(classification_report(y_test, predictions_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T13:26:37.938369Z",
     "start_time": "2019-10-10T13:26:37.933383Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T13:01:20.151Z"
    }
   },
   "outputs": [],
   "source": [
    "svc_pipeline = Pipeline([('vect', tfidf), ('clf', SVC(random_state=1))])\n",
    "\n",
    "param_grid_svm = {\n",
    "                  'vect__use_idf': [True, False],\n",
    "                  'vect__ngram_range': [(1,2)],\n",
    "                  'vect__stop_words': [stopwords, None],\n",
    "                  'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                  'clf__C': [0.1, 1, 10, 100],\n",
    "                  'clf__kernel': ['linear', 'rbf', 'poly'],\n",
    "                  'clf__gamma': [0.1, 1, 10, 100],\n",
    "                  'clf__degree': [0, 1, 2, 3, 4, 5, 6]\n",
    "                 }\n",
    "\n",
    "grid_svc = RandomizedSearchCV(svc_pipeline, param_grid_svm, n_iter=50, cv=3, verbose=2, n_jobs=-1)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_svc.best_score_))\n",
    "print(\"Best parameters: \", grid_svc.best_params_)\n",
    "print(\"Best estimator: \", grid_svc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T13:01:20.754Z"
    }
   },
   "outputs": [],
   "source": [
    "svm = grid_svc.best_estimator_\n",
    "svm_fit = svm.fit(X_train, y_train)\n",
    "score_svm = svm.score(X_test, y_test)\n",
    "predictions_svm = svm.predict(X_test)\n",
    "print(confusion_matrix(y_test, predictions_svm))\n",
    "print(classification_report(y_test, predictions_svm))\n",
    "print(\"The algorithm has reached an accuracy of: {:.2f}%\".format(score_svm*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T13:01:28.476Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T13:01:29.092Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([('vect', tfidf), ('clf', RandomForestClassifier(random_state=1))])\n",
    "param_grid_rf = {'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': np.linspace(1, 32, 32, endpoint=True),\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]}\n",
    "\n",
    "grid_rf = RandomizedSearchCV(rf_pipeline, param_grid_rf, cv=3, n_iter=1000 , verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_rf.best_score_))\n",
    "print(\"Best parameters: \", grid_rf.best_params_)\n",
    "print(\"Best estimator: \", grid_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T13:01:30.295Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = grid_rf.best_estimator_\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "predictions_rf = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test, predictions_rf))\n",
    "print(classification_report(y_test, predictions_rf))\n",
    "print(\"The algorithm has reached an accuracy of: {:.2f}%\".format(\n",
    "    accuracy_score(y_test, predictions_rf)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T13:01:32.099Z"
    }
   },
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "478.8px",
    "left": "1175.6px",
    "right": "20px",
    "top": "124px",
    "width": "341.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
