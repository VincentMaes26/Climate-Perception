{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Get dataframe from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "#tweets_today = pd.read_csv(r\"C:\\stage\\project\\datasets\\tweets{}.csv\".format(datetime.date.today()))\n",
    "tweets_today = pd.read_csv(r\"C:\\stage\\project\\datasets\\tweets2019-09-24.csv\")\n",
    "tweets_today = tweets_today.head(5000)\n",
    "# Gets tweet texts\n",
    "tweets_today_text = tweets_today[\"tweet\"]\n",
    "tweets_today.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def filter_stopwords(word):\n",
    "    if word in stopwords.words('english'):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# Extract tweet column from df\n",
    "df_text = tweets_today[\"tweet\"]\n",
    "\n",
    "for tweet in df_text:\n",
    "    document = tweet.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = filter(filter_stopwords,document)\n",
    "    document = ' '.join(document)\n",
    "    documents.append(document)\n",
    "print(documents[555])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = term frequency, inverse document frequency ()\n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.80)\n",
    "processed_features = vectorizer.fit_transform(documents).toarray()\n",
    "print(processed_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Clustering with K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3, init='random', n_init=10, max_iter=1000, tol=1e-04, random_state=1)\n",
    "y_km = km.fit_predict(processed_features)\n",
    "print(y_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=5000, n_features=2000, centers=3, cluster_std=0.5, shuffle= True, random_state=0)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c='white', marker='o', edgecolor='black', s=50)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.scatter(X[y_km == 0, 0], X[y_km == 0, 1], s=20, c='lightgreen', marker='s', edgecolor='black', label='cluster1' )\n",
    "plt.scatter(X[y_km == 1, 0], X[y_km == 1, 1], s=20, c='orange', marker='o', edgecolor='black', label='cluster2' )\n",
    "plt.scatter(X[y_km == 2, 0], X[y_km == 2, 1], s=20, c='lightblue', marker='v', edgecolor='black', label='cluster3' )\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker='*', c='red', edgecolor=\"black\", label=\"centroids\")\n",
    "\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this clustering algorithm didn't work well. The problem is that with unsupervised learning for sentiment analysis there is no clear threshhold for the algorithm to separate the points from eachother"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Applying Naive Bayes algorithm to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
