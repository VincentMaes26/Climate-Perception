{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up dataframes + preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of tweet texts\n",
    "def format_tweet(tweet):\n",
    "    processed_tweet=\"\"\n",
    "    if \"RT @\" or \"RT\" not in tweet.full_text:\n",
    "        for word in tweet.full_text.split():\n",
    "            \n",
    "            # Removing URL from tweet\n",
    "            processed_word = re.sub('([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', word)\n",
    "            \n",
    "            # Remove all the special characters\n",
    "            processed_word = re.sub(r'\\W', '', processed_word)\n",
    "\n",
    "            # remove all single characters\n",
    "            processed_word = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_word)\n",
    "\n",
    "            # Remove single characters from the start\n",
    "            processed_word = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_word) \n",
    "\n",
    "            # Substituting multiple spaces with single space\n",
    "            processed_word = re.sub(r'\\s+', '', processed_word, flags=re.I)\n",
    "\n",
    "            # Removing prefixed 'b'\n",
    "            processed_word = re.sub(r'^b\\s+', ' ', processed_word)\n",
    "\n",
    "            # Converting to Lowercase\n",
    "            processed_word = processed_word.lower()\n",
    "            processed_tweet= processed_tweet+\" \"+processed_word\n",
    "\n",
    "        return processed_tweet #+\"~ \"+ str(tweet.created_at)\n",
    "            \n",
    "    else:\n",
    "        return\n",
    "    \n",
    "\n",
    "# Handles limit exception from twitter API\n",
    "def limit_handler(cursor : tweepy.cursor):\n",
    "    while True:\n",
    "        try:\n",
    "            yield cursor.next()\n",
    "            \n",
    "        except tweepy.RateLimitError:\n",
    "            print(tweepy.RateLimitError)\n",
    "            time.sleep(15 * 60)\n",
    "            \n",
    "        except StopIteration:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting tweets from Twitter API and saves them to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticates and connects to API\n",
    "auth = tweepy.OAuthHandler(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
    "auth.set_access_token(creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "def store_tweets_to_csv(start_date, end_date  ):\n",
    "    # Gets tweets based on query\n",
    "    query = \"#heatwave -filter=retweets\"\n",
    "    cursor = limit_handler(tweepy.Cursor(api.search, q= query, lang=\"en\", since=start_date,  until=end_date, tweet_mode='extended').items(5000))\n",
    "    \n",
    "    # Filters out None objects\n",
    "    cursor = list(filter(None,cursor))\n",
    "    \n",
    "    # Preprocesses the tweet texts\n",
    "    tweets = [format_tweet(tweet) for tweet in cursor]\n",
    "    tweets = list(filter(None, tweets))\n",
    "\n",
    "    # Gets username and creation date of tweet\n",
    "    usernames = [tweet.user.name for tweet in cursor]\n",
    "    usernames[0]\n",
    "    creation_dates = [tweet.created_at for tweet in cursor]\n",
    "        \n",
    "\n",
    "    list_for_dataframe = list(zip(tweets,usernames, creation_dates))\n",
    "\n",
    "    df = pd.DataFrame(list_for_dataframe, columns=[\"tweet\",\"username\", \"creation date\"])\n",
    "    month = start_date.split('-')[1]\n",
    "    day = start_date.split('-')[2]\n",
    "    if df.shape[0] == 0:\n",
    "        print(\"There has been an error. Dataframe tweets_week{}-{} is empty\".format(day, month))\n",
    "    else:\n",
    "        print(\"The dataframe tweets_week{}-{} has been stored in the datasets folder\".format(day, month))\n",
    "        df.to_csv(r'D:\\dev\\python\\Climate-Perception\\datasets\\tweets_week{}-{}'.format(day, month))\n",
    "                                                                                                               \n",
    "                                                                                                                                          \n",
    "# Can't get tweets from longer than a week ago without premium twitter api\n",
    "#store_tweets_per_year('2016-01-01', '2016-12-31')\n",
    "#store_tweets_per_year('2017-01-01', '2017-12-31')\n",
    "#store_tweets_per_year('2018-01-01', '2018-12-31')\n",
    "\n",
    "store_tweets_to_csv('2019-09-16', '2019-09-22')\n",
    "\n",
    "                                                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Sentiment analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textblob_tweets = [TextBlob(tweet) for tweet in tweets]\n",
    "sentiment_tweets = [[round(tweet.sentiment.polarity,2) for tweet in textblob_tweets]\n",
    "zipped_list = list(zip(creation_dates, tweets, sentiment_tweets, usernames ))\n",
    "#sentiment_values = [[round(tweet.sentiment.polarity,2), tweet.split(\"~\")[0], tweet.split(\"~\")[1]] for tweet in sentiment_tweets]\n",
    "\n",
    "sentiment_df = pd.DataFrame(zipped_list, columns=[\"Creation Date\", \"tweet\", \"Sentiment\", \"Username\"])\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count = sentiment_df[\"polarity\"].value_counts()\n",
    "x = sentiment_count.index\n",
    "y = sentiment_count.values\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x, y, alpha=0.8)\n",
    "plt.yticks(np.arange(min(y), max(y), step=2))\n",
    "plt.xticks(plt.xticks()[0], rotation=65)\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"Amount of tweets\")\n",
    "plt.xlabel(\"Sentiment polarity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this sentiment analysis is very basic and not very accurate\n",
    "This analysis would point out that most people who tweet about heatwaves, have a possitive or neutral sentiment about it. \n",
    "* sentiment polarity < 0 &nbsp;&nbsp;&nbsp; => negative \n",
    "* sentiment polarity > 0 &nbsp;&nbsp;&nbsp; => positive \n",
    "* sentiment polarity = 0 &nbsp;&nbsp;&nbsp; => neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Sentiment analysis with SciKit Learn (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization (reduces words to dictionary root form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for tweet in tweet_texts:\n",
    "    document = tweet.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize words, filter stopwords and initialize training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = term frequency, inverse document frequency ()\n",
    "vectorizer = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.80, stop_words=stopwords.words('english'))\n",
    "processed_features = vectorizer.fit_transform(documents).toarray()\n",
    "X, y = processed_features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
